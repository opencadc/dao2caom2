working_directory: /usr/src/app
# the netrc_filename value must be a fully-qualified name
# netrc_filename: /usr/src/app/netrc
# this is the name of the proxy certificate file supplied 
# to the container. It must be a fully-qualified name. 
# One of netrc_filename or proxy_filename must have a value.
proxy_file_name: /usr/src/app/cadcproxy.pem
# if the 'store' task type is listed, ensure the file being stored
# has a different md5 checksum than the file at CADC before 
# transferring the file.
store_modified_files_only: True 
# If cleanup_files_when_storing is True, and the STORE task_type is
# specified, move files that are successfully transferred to CADC
# to the cleanup_success_destination when the transfer is done, and
# to the cleanup_failure_destination when the transfer fails, or the
# file fails FITS verification.
cleanup_files_when_storing: True
cleanup_success_destination: /data/success
cleanup_failure_destination: /data/failure
# - modifies entries on sc2.canfar.net
# operational value is ivo://cadc.nrc.ca/ams
resource_id: ivo://cadc.nrc.ca/ams
# resource_id: ivo://cadc.nrc.ca/sc2repo
# queries files by date
tap_id: ivo://cadc.nrc.ca/ad
# file that contains the list of entries to process
todo_file_name: todo.txt
# values True False
# when True, the application will look for files with
# .fits, .gz, .json endings as defining the work to be 
# done.
use_local_files: True
# Specify locations in which
# to search for files. This is a YAML list. It may be a list of
# length 1. If use local_files is set to True, it may be set to 
# the same value as the working_directory.
data_sources:
  - /data
# specify a list of the file extensions to be recognized for
# processing by the pipeline. e.g. '.fits', '.fits.fz', '.hdf5'
data_source_extensions:
  - .fits
# values DEBUG INFO WARNING ERROR
logging_level: INFO
# values True False
log_to_file: True
# fully qualified name for a directory to write log files
log_file_directory: /usr/src/app/logs
# the filename where success logs are written
#
# this file is created in the logs directory, default is 'success_log.txt'
success_log_file_name: success_log.txt
# the filename where failure logs are written
#
# this file is created in the logs directory, default is 'failure_log.txt'
failure_log_file_name: failure_log.txt
# the filename where retry ids are written. This file is only written
# if use_local_files is False.
#
# this file is created in the logs directory, default is 'retries.txt'
retry_file_name: retries.txt
retry_failures: True
retry_count: 1
# the filename where pipeline progress is written. This file is
# always written. It's an on-going log of the number of entries processed
# by the pipeline, and is useful when using time-boxed execution.
#
# this file is created in the logs directory, default is 'progress.txt'
progress_file_name: progress.txt
#
collection: DAO
#
# CADC service execution metrics
#
observe_execution: True
observable_directory: /usr/src/app/metrics
# 
# if the pipeline for the collection tracks known failures, that occurs
# in this location
# 
rejected_file_name: rejected.yml
rejected_directory: /usr/src/app/rejected
#
# for information that needs to be persisted between pipeline 
# invocations, defaults to 'state.yml', and is found in 'working_directory'
#
state_file_name: state.yml
#
# if using a state file to time-box execution chunks, this is 
# the interval, in minutes, that define the start and end of the 
# time-box.
#
interval: 600
# how to control the work that gets done
# possible values are:
# - scrape - read the headers, write out the xml - kind of a test mode
# - store - calls cadc-data to put a file from local storage (use_local_files must be set to True)
# - ingest - read the headers, write the xml to the endpoint found by resource_id
# - modify - create a footprint and previews for an existing CAOM model record
# - pull - put a file retrieved from a URL
task_types: 
  - store
  - ingest
  - modify
features:
  supports_latest_client: True
